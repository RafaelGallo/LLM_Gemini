{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Modelo 1 LLM - Gemma**"
      ],
      "metadata": {
        "id": "Iq9yli45xS1K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8LwdgHouvSN"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "-S-TIUeRv0BV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")"
      ],
      "metadata": {
        "id": "MgDkkZXGuz1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\")"
      ],
      "metadata": {
        "id": "0dDaCI2wuz4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Write me a poem about Machine Learning.\""
      ],
      "metadata": {
        "id": "LokMtYsquz8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(input_text, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "FVfTShpAxbb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(**input_ids)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "1LkNhXKMu4X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LLM Gemini**"
      ],
      "metadata": {
        "id": "yJxOxixAwLoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install accelerate\n",
        "from transformers import pipeline, set_seed\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "opHXwGaFu4d2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline('text2text-generation', model='describeai/gemini')"
      ],
      "metadata": {
        "id": "VuIJnQcvvJTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code = \"print('hello world!')\""
      ],
      "metadata": {
        "id": "CumYpotGvJXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = summarizer(code, max_length=100, num_beams=3)\n",
        "print(\"Summarized code: \" + response[0]['generated_text'])"
      ],
      "metadata": {
        "id": "Ax-A2-TCwusc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "6hEMkB2iw9MW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\"text2text-generation\", model=\"describeai/gemini\")"
      ],
      "metadata": {
        "id": "FdzAX0vEw-fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
      ],
      "metadata": {
        "id": "KNLyrRMSw-j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer2 = AutoTokenizer.from_pretrained(\"describeai/gemini\")"
      ],
      "metadata": {
        "id": "49ioBzNBw-m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"describeai/gemini\")"
      ],
      "metadata": {
        "id": "-pXQpDPqxC5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text2 = \"Write me a poem about Machine Learning.\"\n",
        "input_ids = tokenizer2(input_text2, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "ktiobZPrxE9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(**input_ids)\n",
        "print(tokenizer2.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "1vsIEu2qxIJQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}